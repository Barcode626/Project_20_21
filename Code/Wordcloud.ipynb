{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_wordcloud.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyM/9CZf6u3ydfjrqyarzzK9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"uRk505NpN_7P"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import csv\n","import re\n","import shutil\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Code\")\n","import cleaning_tweets\n","from cleaning_tweets import preprocess, preprocess_hg\n","os.chdir(\"/content/drive/MyDrive/Data\")\n","path_txt = \"/content/drive/MyDrive/Data/wordcloud/\"\n","\n","!pip install wordle==0.2.1\n","from wordle import Wordle, export_wordcloud\n","import pathlib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x40RGtt5Z4fn"},"source":["WordCloud for hashtags"]},{"cell_type":"code","metadata":{"id":"DXVRiQxazdc2"},"source":["df_mini = pd.read_csv('data_with_labels_mini.csv', header = None, dtype = str, encoding = 'utf-8-sig', engine='python')\n","df_mini.columns = ['date', 'id', 'text', 'hashtags', \\\n","                                    'state', 'county and city',\\\n","                                    'retweet_count', 'favorite_count',\\\n","                                    'text_id', 'st_labels', 'st_scores', 'aspects']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DeHCIkB5Z38A"},"source":["txtfile = open('COVID19_AIDR_Keywords.txt')\n","lines = txtfile.readlines()\n","ids = []\n","for line in lines:\n","    line = line.split(',')\n","    for i in line:\n","        i = re.sub(r'[^a-zA-Z]', '', i)\n","        if i != '':\n","            i = i.lower()\n","            ids.append(i.strip())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_kPc4ouv_ao"},"source":["def encodetoken_hg(tokens, keyword):\n","    comment_words = ''\n","    if keyword == 0:\n","        # remove covid related tags\n","        token_lst = [token.strip() for token in tokens if token.strip() not in ids]\n","        comment_words += \" \".join(token_lst)+\" \"\n","    else:\n","        token_lst = [token.strip() for token in tokens]\n","        comment_words += \" \".join(token_lst)+\" \"\n","    return comment_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G27sWdLxzq6j"},"source":["# hashtags in 4million data\n","df1 = df_mini.copy()\n","df1.dropna(subset=['hashtags'], axis=0, how='any', inplace=True)\n","df1.reset_index(inplace=True, drop=True)\n","df1.loc[:, 'preprocess_text'] = df1['hashtags'].apply(preprocess_hg)\n","df1.loc[:, 'text_nonstop_cvd'] = df1['preprocess_text'].apply(lambda r: encodetoken_hg(r, 1))\n","df1.loc[:, 'text_nonstop_noncvd'] = df1['preprocess_text'].apply(lambda r: encodetoken_hg(r, 0))\n","df1['text_nonstop_cvd'].to_csv('hashtags_4m_cvd.txt', header=None, index=False, sep=\"\\t\")\n","df1['text_nonstop_noncvd'].to_csv('hashtags_4m_noncvd.txt', header=None, index=False, sep=\"\\t\")\n","w = Wordle(random_state=5678)\n","w.generate_from_file('hashtags_4m_cvd.txt', outfile='hashtags_4m_cvd.png')\n","w.generate_from_file('hashtags_4m_noncvd.txt', outfile='hashtags_4m_noncvd.png')\n","os.remove('hashtags_4m_cvd.txt')\n","os.remove('hashtags_4m_noncvd.txt')\n","shutil.move('hashtags_4m_cvd.png', path_txt+ 'hashtags_4m_cvd.png')\n","shutil.move('hashtags_4m_noncvd.png', path_txt+ 'hashtags_4m_noncvd.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWwxM7K-OOn9"},"source":["WordCloud for different aspects"]},{"cell_type":"code","metadata":{"id":"uSo6HzYX5kvj"},"source":["df = pd.read_csv('ap_textid_mini.csv', dtype = str, encoding = 'utf-8-sig', engine='python')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W47UmrT-OIOS"},"source":["def encodetoken(tokens):\n","    comment_words = ''\n","    token_lst = [token for token in tokens]\n","    comment_words += \" \".join(token_lst)+\" \"\n","    return comment_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrbGyNEqr83H"},"source":["w = Wordle(random_state=5678)\n","labels = ['negative', 'positive']\n","for idx in range(0, 9):\n","    df1 = df[df['aspects_idx']==str(idx)].copy()\n","    for i in range(2):\n","        df2 = df1[df1['st_labels']==str(i)].copy()\n","        df2.loc[:, 'preprocess_text'] = df2['text'].apply(preprocess)\n","        df2.loc[:, 'text_nonstop'] = df2['preprocess_text'].apply(lambda r: encodetoken(r))\n","        filename = 'aspect_' + labels[i] + str(idx)\n","        df2['text_nonstop'].to_csv(filename +'.txt', header=None, index=False, sep=\"\\t\")\n","        w.generate_from_file(filename + '.txt', outfile = filename + '.png')\n","        os.remove(filename + '.txt')\n","        shutil.move(filename + '.png', path_txt + filename + '.png')"],"execution_count":null,"outputs":[]}]}