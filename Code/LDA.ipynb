{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"7_LDA.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bdHYQQPtDEeL"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import csv\n","import time\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Code\")\n","import cleaning_tweets as ct\n","from cleaning_tweets import getidx, preprocess, dealing_topics, generate_aspects, clean_aspects\n","\n","# Gensim\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","import gensim.corpora as corpora\n","from gensim.models import CoherenceModel\n","\n","# NLTK\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","from nltk.stem.porter import * \n","\n","os.chdir(\"/content/drive/MyDrive/Data\")\n","model_path = \"/content/drive/MyDrive/Code/Model/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PFg7MQhgDkW1"},"source":["# Training Process"]},{"cell_type":"code","metadata":{"id":"-JoXj5SuMSww"},"source":["df = pd.read_csv('textid.csv', engine='python', encoding = 'utf-8-sig', dtype=str, header = None)\n","df.columns = ['text', 'text_id']\n","\n","result = [i for i in range(0, df.shape[0], 10)]\n","print(f'The length of samples: {len(result)}')\n","\n","train_data = pd.DataFrame()\n","for i in range(0, df.shape[0], 10):\n","    train_data = df.iloc[i:i+1, :].copy()\n","    # train_data.to_csv('./topic/trainset4lda_590000.csv', mode='a', header=None, index=False, encoding='utf-8-sig')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ij1TKerkDLT3","executionInfo":{"status":"ok","timestamp":1630395101832,"user_tz":-480,"elapsed":105164,"user":{"displayName":"C XXX","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi80UhcWrrQsYptKdXzHpzrVRaRJG6wsKSZq7pO=s64","userId":"16747379684365591919"}},"outputId":"5caf46ec-fc89-41b2-a490-dfac1ff3cc07"},"source":["# get the train data to train lda\n","train_data = pd.read_csv('./topic/trainset4lda_590000.csv', engine='python', encoding = 'utf-8-sig', dtype=str, header = None)\n","train_data.columns = ['text', 'text_id']\n","train_data.drop_duplicates(['text_id'], keep='first', ignore_index=True, inplace=True)\n","print(f'Train dataset has {train_data.shape[0]} rows.')\n","\n","# Preprocess\n","train_data.loc[:, 'preprocess_text'] = train_data['text'].apply(preprocess)\n","# Construct a dictionary\n","dictionary = gensim.corpora.Dictionary(train_data.loc[:, 'preprocess_text'])\n","# Get a bag-of-words representation\n","bow_corpus = train_data['preprocess_text'].apply(dictionary.doc2bow)\n","\n","# Train the model\n","lda_model =  gensim.models.LdaModel(bow_corpus, num_topics=30, id2word=dictionary, \\\n","                                                                            passes=20, random_state = 400,)\n","\n","# # Save Models, Dictionary and Corpus\n","# lda_model.save(fname= model_path + \"LDA_Model\")\n","# dictionary.save(model_path + \"dictionary.dict\") \n","# corpora.MmCorpus.serialize(model_path + \"bow_corpus.mm\", bow_corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset has 590000 rows.\n"]}]},{"cell_type":"markdown","metadata":{"id":"hK7gneRED8uR"},"source":["# Apply model to Testset"]},{"cell_type":"code","metadata":{"id":"cgsnl1iFDNFl","executionInfo":{"status":"ok","timestamp":1631523726888,"user_tz":-480,"elapsed":4,"user":{"displayName":"C XXX","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi80UhcWrrQsYptKdXzHpzrVRaRJG6wsKSZq7pO=s64","userId":"16747379684365591919"}}},"source":["# load the model, dictionary and corpus\n","lda_model = gensim.models.LdaModel.load(model_path + \"LDA_Model\")\n","dictionary = gensim.corpora.Dictionary.load(model_path + \"dictionary.dict\")\n","bow_corpus = corpora.MmCorpus(model_path + \"bow_corpus.mm\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftL6pXtKyAY5","executionInfo":{"status":"ok","timestamp":1631523731520,"user_tz":-480,"elapsed":403,"user":{"displayName":"C XXX","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi80UhcWrrQsYptKdXzHpzrVRaRJG6wsKSZq7pO=s64","userId":"16747379684365591919"}}},"source":["topics = lda_model.print_topics(num_topics = 30, num_words = 10)\n","df_topic = pd.DataFrame(columns=[\"topic_index\", \"topics\"])\n","for idx, topic in topics:\n","    df_topic.loc[idx, \"topic_index\"] = idx\n","    df_topic.loc[idx, \"topics\"] = topic\n","# df_topic.to_excel(\"./topic/original_topics.xlsx\", index=False)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Swg6w6Wcz-ml","executionInfo":{"status":"ok","timestamp":1630484995824,"user_tz":-480,"elapsed":333,"user":{"displayName":"C XXX","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi80UhcWrrQsYptKdXzHpzrVRaRJG6wsKSZq7pO=s64","userId":"16747379684365591919"}},"outputId":"28f19c16-ebcf-42ed-be90-2290421166cf"},"source":["# df_topic = pd.read_excel(\"./topic/original_topics.xlsx\")\n","topicidx_to_aspect = {'0': 'lockdown', '1': 'government', '2': 'lockdown', \\\n","                                '3': 'protective measures', '4': 'china', '8': 'lockdown', \\\n","                                '10': 'lockdown', '11': 'treatment', '12': 'support measures', \\\n","                                '13': 'protective measures', '15': 'lockdown', '16': 'support measures', \\\n","                                '18': 'support measures', '20': 'quarantine', '21': 'spread', \\\n","                                '23': 'information', '25': 'support measures', '26': 'lockdown', \\\n","                                '27': 'spread', '28': 'protective measures', '29': 'government'}\n","\n","for idx, ap in topicidx_to_aspect.items():\n","    df_topic.loc[int(idx), 'aspects'] = ap\n","df_topic.reset_index(inplace=True, drop=True)\n","\n","df_discard = df_topic[df_topic.isnull().T.any()]\n","df_discard.reset_index(inplace=True, drop=True)\n","\n","df_aspect = df_topic.dropna(subset=['aspects'], axis=0, how='any').copy()\n","df_aspect.reset_index(inplace=True, drop=True)\n","aspect_lst = list(np.unique(df_aspect['aspects'].astype(str)))\n","aspect_to_idx = {}\n","for idx, ap in enumerate(aspect_lst):\n","    aspect_to_idx[ap] = idx\n","\n","df_aspect_idx = pd.DataFrame(aspect_to_idx.keys(), columns=['aspects'])\n","df_aspect_idx.loc[:, 'aspect_idx'] = aspect_to_idx.values()\n","# df_aspect_idx.to_excel(\"./topic/aspect2idx.xlsx\", index=False)\n","# df_discard.to_excel(\"./topic/discard.xlsx\", index=False)\n","print(f\"It has {len(np.unique(df_aspect['aspects']))} aspects: \\n {aspect_to_idx}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It has 9 aspects: \n"," {'china': 0, 'government': 1, 'information': 2, 'lockdown': 3, 'protective measures': 4, 'quarantine': 5, 'spread': 6, 'support measures': 7, 'treatment': 8}\n"]}]},{"cell_type":"code","metadata":{"id":"8k3fCYIKDb0q"},"source":["# topicidx_to_aspect, aspect_to_idx, idx_to_aspect = getidx()\n","\n","# label all the text using lda\n","df_chunk = pd.read_csv('textid.csv', header = None, chunksize=300000, dtype = str, encoding = 'utf-8-sig', engine='python')\n","\n","file_counts = 0\n","for chunk in df_chunk:\n","    chunk_time = time.time()\n","    chunk.columns = [\"text\", \"text_id\"]\n","    chunk.dropna(subset=[\"text_id\"], axis=0, how='any', inplace=True)\n","    chunk.reset_index(inplace=True, drop=True)\n","    \n","    chunk.loc[:, \"preprocess_text\"] = chunk[\"text\"].astype(str).apply(preprocess)\n","    chunk.loc[:, \"topics\"] = chunk[\"preprocess_text\"].apply(dealing_topics)\n","    chunk.loc[:, \"aspect\"] = chunk[\"topics\"].apply(generate_aspects)\n","    chunk.loc[:, \"aspects\"] = chunk[\"aspect\"].astype(str).apply(clean_aspects)\n","\n","    chunk.drop(labels=[\"text\", \"preprocess_text\", \"topics\", \"aspect\"], axis=1, inplace=True) \n","    chunk.to_csv(\"textid_ap.csv\", mode='a', header=None, index=False, encoding='utf-8-sig') \n","\n","    print(f'File {file_counts}:')\n","    print(f'time cost: {(time.time()-chunk_time)//60} minutes')\n","    file_counts += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dj3bt5hxLpnR"},"source":[""],"execution_count":null,"outputs":[]}]}